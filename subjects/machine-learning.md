# ğŸ¤– Machine Learning (DA Paper)

![Weightage](https://img.shields.io/badge/Weightage-8â€“12%20marks%20(DA)-red)
![Priority](https://img.shields.io/badge/Priority-Very%20High%20(DA)-red)

---

## ğŸ“‹ Topic Checklist

### ğŸ”· Foundations
- [ ] Types of ML: supervised, unsupervised, semi-supervised, reinforcement learning
- [ ] Training, validation, and test sets
- [ ] Bias-variance tradeoff
- [ ] Overfitting and underfitting
- [ ] Regularization: L1 (Lasso), L2 (Ridge), Elastic Net
- [ ] Cross-validation: k-fold, leave-one-out (LOOCV)
- [ ] Feature engineering and feature selection

### ğŸ”· Supervised Learning â€” Regression
- [ ] Linear regression â€” least squares estimation
- [ ] Multiple linear regression
- [ ] Polynomial regression
- [ ] Ridge and Lasso regression
- [ ] Logistic regression â€” sigmoid function, binary and multi-class
- [ ] Maximum Likelihood Estimation (MLE)

### ğŸ”· Supervised Learning â€” Classification
- [ ] Decision trees â€” entropy, information gain, Gini impurity
- [ ] Random forests â€” bagging
- [ ] Naive Bayes classifier â€” Bayes theorem, class conditional independence
- [ ] k-Nearest Neighbors (k-NN)
- [ ] Support Vector Machines (SVM) â€” hyperplane, margin, kernel trick
  - [ ] Hard margin vs soft margin
  - [ ] Kernel functions: linear, polynomial, RBF/Gaussian
- [ ] Perceptron learning rule
- [ ] Multi-layer perceptron (MLP)

### ğŸ”· Unsupervised Learning
- [ ] k-Means clustering â€” algorithm, convergence, limitations
- [ ] Hierarchical clustering â€” agglomerative, divisive
- [ ] DBSCAN (density-based)
- [ ] Gaussian Mixture Models (GMM) and EM algorithm
- [ ] Principal Component Analysis (PCA) â€” dimensionality reduction
- [ ] Singular Value Decomposition (SVD)
- [ ] Autoencoders (basics)

### ğŸ”· Neural Networks
- [ ] Neuron model â€” activation function, weights, bias
- [ ] Activation functions: Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax
- [ ] Feedforward neural networks
- [ ] Backpropagation â€” chain rule for gradients
- [ ] Gradient descent variants: Batch, SGD, Mini-batch
- [ ] Optimizers: Momentum, RMSProp, Adam
- [ ] Vanishing/exploding gradient problem
- [ ] Batch normalization, Dropout
- [ ] Convolutional Neural Networks (CNNs) â€” convolution, pooling, filters
- [ ] Recurrent Neural Networks (RNNs) â€” sequence modeling, LSTM, GRU

### ğŸ”· Evaluation Metrics
- [ ] Confusion matrix: TP, TN, FP, FN
- [ ] Accuracy, Precision, Recall, F1-score
- [ ] ROC curve and AUC
- [ ] Mean Squared Error (MSE), RMSE, MAE
- [ ] RÂ² (coefficient of determination)
- [ ] Log loss / cross-entropy loss

### ğŸ”· Optimization
- [ ] Gradient descent â€” learning rate, convergence
- [ ] Convex and non-convex optimization
- [ ] Local vs global minima
- [ ] Stochastic vs batch optimization

### ğŸ”· Model Selection & Validation
- [ ] Bias-variance decomposition
- [ ] Learning curves
- [ ] Hyperparameter tuning: grid search, random search, Bayesian optimization
- [ ] Ensemble methods: bagging, boosting, stacking
- [ ] AdaBoost, Gradient Boosting, XGBoost (conceptual)

---

## âš¡ Key Formulas

### Linear Regression
```
Å· = XÎ²
Î² = (X^T X)â»Â¹ X^T y   (Ordinary Least Squares)

MSE = (1/n) Î£ (yáµ¢ âˆ’ Å·áµ¢)Â²
```

### Logistic Regression
```
Ïƒ(z) = 1 / (1 + e^(-z))   (sigmoid)
P(y=1|x) = Ïƒ(w^T x + b)

Loss (binary cross-entropy): -[y log(Å·) + (1-y) log(1-Å·)]
```

### Decision Tree Entropy
```
Entropy: H(S) = -Î£ páµ¢ logâ‚‚(páµ¢)
Information Gain: IG(S,A) = H(S) - Î£ (|Sáµ¥|/|S|) H(Sáµ¥)
Gini Impurity: G(S) = 1 - Î£ páµ¢Â²
```

### Naive Bayes
```
P(C|x) âˆ P(C) Ã— Î  P(xáµ¢|C)
```

### SVM
```
Decision boundary: w^T x + b = 0
Margin = 2/||w||
Maximize margin â†” Minimize ||w||Â²/2 subject to yáµ¢(w^T xáµ¢ + b) â‰¥ 1
```

### PCA
```
1. Center the data (subtract mean)
2. Compute covariance matrix Î£
3. Compute eigenvectors and eigenvalues of Î£
4. Project data onto top-k eigenvectors (principal components)
Variance explained by component = Î»áµ¢ / Î£Î»
```

### Evaluation Metrics
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

### Bias-Variance Tradeoff
```
Expected error = BiasÂ² + Variance + Irreducible noise

High bias â†’ underfitting (model too simple)
High variance â†’ overfitting (model too complex)
```

---

## âŒ Common Mistakes

| Mistake | Correct Understanding |
|---|---|
| High accuracy = good model | Consider class imbalance â€” use F1, AUC instead |
| More features = better | Feature selection prevents overfitting |
| Gradient descent always converges | Only guaranteed for convex functions; use adaptive LR |
| Cross-entropy = MSE | Cross-entropy for classification; MSE for regression |
| SVM only for linear separation | Kernel trick enables non-linear boundaries |
| k-NN requires training phase | k-NN is lazy learner â€” no explicit training |

---

## ğŸ“Š PYQ Frequency Analysis (GATE DA)

| Topic | Frequency | Marks Range |
|---|:---:|:---:|
| Supervised learning (regression/classification) | Very High | 2â€“4 |
| Neural networks / backpropagation | High | 2â€“3 |
| Evaluation metrics | High | 1â€“2 |
| Clustering algorithms | High | 1â€“2 |
| PCA / dimensionality reduction | Medium | 1â€“2 |
| SVM | Medium | 1â€“2 |
| Bayes / Naive Bayes | High | 1â€“2 |

---

## ğŸ¯ Confidence Tracker

| Topic | Confidence | Last Revised |
|---|:---:|---|
| Linear/Logistic Regression | â˜ Low / â˜ Med / â˜ High | |
| Decision Trees / Random Forest | â˜ Low / â˜ Med / â˜ High | |
| SVM | â˜ Low / â˜ Med / â˜ High | |
| Neural Networks | â˜ Low / â˜ Med / â˜ High | |
| Clustering | â˜ Low / â˜ Med / â˜ High | |
| PCA / Dimensionality Reduction | â˜ Low / â˜ Med / â˜ High | |
| Evaluation Metrics | â˜ Low / â˜ Med / â˜ High | |
